{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping and Acquistion \n",
    "\n",
    "**Datasets**\n",
    "\n",
    "The datasets are acquired from (1) Spotify API (Spotipy) and (2) Billboard Hot 100 2020 \n",
    "\n",
    "For Spotify, the official \"Top Tracks of 2020 Singapore\" were chosen as below:\n",
    "\n",
    "https://open.spotify.com/playlist/37i9dQZF1DX8X0dOzodgNr\n",
    "\n",
    "\n",
    "For Billboard Hot 100, the data was taken from Wikipedia:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping Playlists from Spotify and Data Wrangling** \n",
    "\n",
    "In the following section the dataset from Spotify are acquired, and then undergoes a data wrangling process to get the necessary info on track titles, genre, audio features (danceability, energy,valence,tempo...etc).\n",
    "\n",
    "Subsequently the lyrics from Genius.com (embedded within Spotify) are also embedded into the dataset. It should be noted some tracks had lyrics that were unavailable.\n",
    "\n",
    "Python functions were used extensively in this process to get the desired dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages Required to Run this Notebook\n",
    "\n",
    "##!pip install requests\n",
    "##!pip install spotipy\n",
    "##!pip install pip install BeautifulSoup\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREDENTIALS for Spotify API\n",
    "cid ='e37f689bb74846d68b1e13ab62a6b036'\n",
    "secret ='06c86b343635490b87896a7ff0333b28'\n",
    "# Passing credentials \n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TAKES PLAYLIST URL AS ARGUMENT\n",
    "def get_playlist_data(url):\n",
    "    uri = []\n",
    "    track = []\n",
    "    duration_ms = []\n",
    "    explicit = []\n",
    "    track_number = []\n",
    "    genre =[]\n",
    "    artist = []\n",
    "    release_date = []\n",
    "    popularity=[]\n",
    "    # search object\n",
    "    one = sp.playlist_items(url)\n",
    "    #CREATING PANDAS DATAFRAME\n",
    "    df1 = pd.DataFrame(one)\n",
    "\n",
    "    #LOOPING ON EACH VALUE one by one to get info out of it & Appending it to List\n",
    "    for i,x in df1['items'].items():\n",
    "        #FOR GENRES\n",
    "        genre_val = sp.artist(x['track']['artists'][0]['external_urls']['spotify'])\n",
    "        track_number.append(x['track']['track_number'])\n",
    "        uri.append(x['track']['uri'])\n",
    "        release_date.append(x['track']['album']['release_date'])\n",
    "        track.append(x['track']['name'])\n",
    "        duration_ms.append(x['track']['duration_ms'])\n",
    "        explicit.append(x['track']['explicit'])\n",
    "        one = sp.album_tracks(x['track']['album']['uri'], offset=0, market='SG')\n",
    "        art = sp.artist_top_tracks(one['items'][0]['artists'][0]['uri'])\n",
    "        artist.append(art['tracks'][0]['album']['artists'][0]['name'])\n",
    "        num = int(np.random.randint(0,len(genre_val['genres'])))\n",
    "        genre.append(genre_val['genres'][num])\n",
    "        popularity.append(x['track']['popularity'])\n",
    "    #Creating another pandas dataframe from the list data extracted in above step\n",
    "    df2 = pd.DataFrame({\n",
    "    'artist':artist,\n",
    "    'popularity':popularity,\n",
    "    'track_uri':uri,\n",
    "    'track':track,\n",
    "    'track_number':track_number,\n",
    "    'genre':genre,\n",
    "    'release_date':release_date,\n",
    "    'duration_ms':duration_ms,\n",
    "    'explicit':explicit,\n",
    "    'artist':artist})\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert output dataframe from the get_album_tracks function\n",
    "def get_track_info(df):\n",
    "    danceability = []\n",
    "    energy = []\n",
    "    key = []\n",
    "    loudness = []\n",
    "    speechiness = []\n",
    "    acousticness = []\n",
    "    instrumentalness = []\n",
    "    liveness = []\n",
    "    valence = []\n",
    "    tempo = []\n",
    "    #Getting audio features from the dataframe extracted in previous step\n",
    "    for i in df['track_uri']:\n",
    "        for x in sp.audio_features(tracks=[i]):\n",
    "            danceability.append(x['danceability'])\n",
    "            energy.append(x['energy'])\n",
    "            key.append(x['key'])\n",
    "            loudness.append(x['loudness'])\n",
    "            speechiness.append(x['speechiness'])\n",
    "            acousticness.append(x['acousticness'])\n",
    "            instrumentalness.append(x['instrumentalness'])\n",
    "            liveness.append(x['liveness'])\n",
    "            valence.append(x['valence'])\n",
    "            tempo.append(x['tempo'])\n",
    "      #Creating another dataframe      \n",
    "    df2 = pd.DataFrame({\n",
    "    'danceability':danceability,\n",
    "    'energy':energy,\n",
    "    'key':key,\n",
    "    'loudness':loudness,\n",
    "    'speechiness':speechiness,\n",
    "    'acousticness':acousticness,\n",
    "    'instrumentalness':instrumentalness,\n",
    "    'liveness':liveness,\n",
    "    'valence':valence,\n",
    "    'tempo':tempo})\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging both dataframe i.e. audio features & Track\n",
    "def merge_frames(df1, df2):\n",
    "    df3 = df1.merge(df2, left_index= True, right_index= True)\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapping Lyrics from genius.com and combining with the previously fetched data\n",
    "def scrape_lyrics(artistname, songname):\n",
    "    artistname2 = str(artistname.replace(' ','-')) if ' ' in artistname else str(artistname)\n",
    "    songname2 = str(songname.replace(' ','-')) if ' ' in songname else str(songname)\n",
    "    #Requesting genius.com with artist name & song name to get the lyrics\n",
    "    page = requests.get('https://genius.com/'+ artistname2 + '-' + songname2 + '-' + 'lyrics')\n",
    "    #Parsing the HTML data\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "    #getting the parsed data\n",
    "    lyrics1 = html.find(\"div\", class_=\"lyrics\")\n",
    "    lyrics2 = html.find(\"div\", class_=\"Lyrics__Container-sc-1ynbvzw-2 jgQsqn\")\n",
    "    if lyrics1:\n",
    "        lyrics = lyrics1.get_text()\n",
    "    elif lyrics2:\n",
    "        lyrics = lyrics2.get_text()\n",
    "    elif lyrics1 == lyrics2 == None:\n",
    "        lyrics = None\n",
    "    #RETURN THE SCRAPPED DATA BACK \n",
    "    return lyrics\n",
    "\n",
    "'''\n",
    "Function Takes Dataframe as argument and then scrapes lyrics from genius.com for respective artist and track\n",
    "'''\n",
    "def merge_lyrics(df):\n",
    "    #Converting artist column to list\n",
    "    artist_list = df['artist'].to_list()\n",
    "    #Enumerating track column to get the index values\n",
    "    for i,track in enumerate(merged_df['track']):\n",
    "        #Calling the scrap function to scrape the data\n",
    "        result = scrape_lyrics(artist_list[i], track)\n",
    "        #If due to some error, result is not coming, don't count that we will handle that in upcoming code\n",
    "        if result != None:\n",
    "            df.loc[i, 'lyrics'] = result\n",
    "        #IF lyrics are not present on genius.com, WRITE DOWN THE UNAVAILABLE MESSAGE\n",
    "        else:\n",
    "            df.loc[i, 'lyrics'] = \"Unavailable\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sending Playlist Link to get the Data\n",
    "df1 = get_playlist_data(\"https://open.spotify.com/playlist/37i9dQZF1DX8X0dOzodgNr\")\n",
    "#Getting Track info i.e. audio features\n",
    "df2 = get_track_info(df1)\n",
    "#Merging both Dataframes\n",
    "merged_df = merge_frames(df1, df2)\n",
    "#Scraping lyrics and appending it to the dataframe created in above step\n",
    "spotify_data = merge_lyrics(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITING THE DATA TO CSV FILE\n",
    "spotify_data.to_csv(\"data/spotify_singapore1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Writing scraped data to sqlite3 database\n",
    "\n",
    "#!pip install db-sqlite3\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "db = sqlite3.connect(\"data/spotify.db\")\n",
    "\n",
    "spotify_data.to_sql(\"spotify\",db,index = False,if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping Billboard Top 100 and Data Wrangling**\n",
    "\n",
    "Here the dataset for Billboard Top 100 2020 was acquired and wrangled to get the desired dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETTING DATA FROM BILLIBOARD\n",
    "html = requests.get('https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2020')\n",
    "\n",
    "#PARSING THE PAGE using beautiful soup\n",
    "html_soup = BeautifulSoup(html.text,'html.parser')\n",
    "table = html_soup.find('table',class_=\"wikitable\")\n",
    "cols = table.find_all(\"td\")\n",
    "out = []\n",
    "for i in cols:\n",
    "    out.append(i.get_text().strip('\"\"').strip(\"\\n\")) \n",
    "    \n",
    "#Extracting out the required fields\n",
    "title = out[::2]\n",
    "artist = out[1::2]\n",
    "\n",
    "#create DataFrame and write output to it\n",
    "dff = pd.DataFrame({'Title':title,'Artist':artist})\n",
    "\n",
    "## witing to sqlite3 database\n",
    "\n",
    "## Connection \n",
    "\n",
    "conn = sqlite3.connect(\"data/billiboard.db\")\n",
    "\n",
    "dff.to_sql(\"billiboard\",conn,index = False,if_exists='replace')##writing to db\n",
    "\n",
    "\n",
    "\n",
    "#Writing Output to the CSV File\n",
    "\n",
    "dff.to_csv(\"data/Billiboard_Data.csv\",index=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
